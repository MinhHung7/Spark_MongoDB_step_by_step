# Spark MLlib FPGrowth
Spark MLlib FPGrowth l√† m·ªôt thu·∫≠t to√°n m·∫°nh m·∫Ω ƒë·ªÉ khai ph√° t·∫≠p m·ª•c th∆∞·ªùng xuy√™n v√† lu·∫≠t k·∫øt h·ª£p (association rules), th∆∞·ªùng d√πng trong c√°c b√†i to√°n nh∆∞:
- G·ª£i √Ω s·∫£n ph·∫©m (nh∆∞ Amazon, Shopee)
- Ph√¢n t√≠ch gi·ªè h√†ng (Market Basket Analysis)
- Khai ph√° quy lu·∫≠t mua s·∫Øm

# üìö Gi·ªõi thi·ªáu ng·∫Øn v·ªÅ FPGrowth trong Spark
- T√™n ƒë·∫ßy ƒë·ªß: Frequent Pattern Growth
- M·ª•c ti√™u: T√¨m ra c√°c t·∫≠p item ph·ªï bi·∫øn xu·∫•t hi·ªán c√πng nhau trong c√°c giao d·ªãch
- L√† m·ªôt ph·∫ßn c·ªßa th∆∞ vi·ªán pyspark.ml.fpm.FPGrowth (thu·ªôc spark.ml, kh√¥ng ph·∫£i spark.mllib c≈©)

# C√†i ƒë·∫∑t v√† chu·∫©n b·ªã m√¥i tr∆∞·ªùng
```bash
!apt-get install openjdk-11-jdk -y
!wget -q https://downloads.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz
!tar xf spark-3.4.1-bin-hadoop3.tgz
!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.4.1-bin-hadoop3"

import findspark
findspark.init()
```
# V√≠ d·ª• ƒë∆°n gi·∫£n v·ªõi FPGrowth
```bash
from pyspark.sql import SparkSession
from pyspark.ml.fpm import FPGrowth

spark = SparkSession.builder.appName("FPGrowth Example").getOrCreate()

# D·ªØ li·ªáu giao d·ªãch
data = [
    (0, ["milk", "bread", "butter"]),
    (1, ["bread", "butter"]),
    (2, ["milk", "bread"]),
    (3, ["milk", "bread", "butter", "apple"]),
    (4, ["bread", "butter"])
]

# T·∫°o DataFrame
df = spark.createDataFrame(data, ["id", "items"])

# Hu·∫•n luy·ªán m√¥ h√¨nh FPGrowth
fpGrowth = FPGrowth(itemsCol="items", minSupport=0.5, minConfidence=0.6)
model = fpGrowth.fit(df)

# In c√°c itemsets th∆∞·ªùng xuy√™n
model.freqItemsets.show()

# In lu·∫≠t k·∫øt h·ª£p
model.associationRules.show()

# D·ª± ƒëo√°n c√°c item c√≥ th·ªÉ mua th√™m cho user m·ªõi
model.transform(df).show()
```
# ƒê·ªãnh d·∫°ng d·ªØ li·ªáu y√™u c·∫ßu b·ªüi Spark MLlib FPGrowth trong th∆∞ vi·ªán pyspark.ml.fpm.FPGrowth:
L√† m·ªôt DataFrame c√≥ 2 c·ªôt:
- ID (tu·ª≥ ch·ªçn): IntegerType ho·∫∑c StringType ‚Äì c√≥ th·ªÉ l√† transaction_id ho·∫∑c user_id.
- Items (b·∫Øt bu·ªôc): m·ªôt c·ªôt ki·ªÉu array<string>, trong ƒë√≥ m·ªói d√≤ng l√† m·ªôt danh s√°ch c√°c item c·ªßa m·ªôt giao d·ªãch.

Gi·∫£ s·ª≠ c√≥ dataframe nh∆∞ sau:

|Acetominifen|Anchovies|Aspirin|Auto Magazines|Bagels|
|------------|---------|-------|--------------|------|
|           1|        0|      0|             0|     0|
|           1|        0|      0|             0|     0|
|           0|        0|      0|             0|     0|
|           0|        0|      0|             0|     0|

Ta chuy·ªÉn v·ªÅ d·∫°ng chu·∫©n b·∫±ng code
```bash
def get_items(row):
    return [item for item in item_columns if row[item] == 1]

# T·∫°o c·ªôt 'items' t·ª´ c√°c c·ªôt one-hot
df_items = df.rdd.zipWithIndex().map(lambda x: (x[1], get_items(x[0]))).toDF(["id", "items"])
df_items.show(truncate=False)
```
- df.rdd: chuy·ªÉn DataFrame sang RDD<Row>
- .zipWithIndex(): g√°n s·ªë th·ª© t·ª± d√≤ng (index) cho m·ªói row ‚Üí Tr·∫£ v·ªÅ RDD g·ªìm c√°c ph·∫ßn t·ª≠ d·∫°ng (row, index)
- .map(lambda x: (x[1], get_items(x[0]))):
  - x[0]: l√† row
  - x[1]: l√† index (id)
- √Åp d·ª•ng get_items(x[0]) ƒë·ªÉ l·∫•y danh s√°ch s·∫£n ph·∫©m c√≥ trong giao d·ªãch ‚Üí T·∫°o tuple (id, [items])
- .toDF(["id", "items"]): chuy·ªÉn RDD k·∫øt qu·∫£ th√†nh l·∫°i DataFrame c√≥ 2 c·ªôt:
  - id: s·ªë th·ª© t·ª± giao d·ªãch
  - items: danh s√°ch s·∫£n ph·∫©m
# Apply Spark MLlib FPGrowth to the formatted data. Mine the set of frequent patterns with the minimum support of 0.1. Mine the set of association rules with the minimum confidence of 0.9.
```bash
fpGrowth = FPGrowth(
    itemsCol="items",
    minSupport=0.1,
    minConfidence=0.9
)

model = fpGrowth.fit(df_items)

# C√°c t·∫≠p item ph·ªï bi·∫øn (frequent itemsets)
model.freqItemsets.show(truncate=False)

# Lu·∫≠t k·∫øt h·ª£p (association rules)
model.associationRules.show(truncate=False)

# D·ª± ƒëo√°n c√°c item c√≥ th·ªÉ g·ª£i √Ω th√™m cho ng∆∞·ªùi d√πng
model.transform(df_items).show(truncate=False)
```
# C√°c functions th∆∞·ªùng g·∫∑p c·ªßa pyspark.ml
1. Transformer

| T√™n                | M√¥ t·∫£                                                   | V√≠ d·ª•                                      |
| ------------------ | ------------------------------------------------------- | ------------------------------------------ |
| `Tokenizer`        | T√°ch vƒÉn b·∫£n th√†nh danh s√°ch t·ª´                         | T·ª´ `"hello world"` ‚Üí `["hello", "world"]`  |
| `RegexTokenizer`   | T√°ch t·ª´ d·ª±a tr√™n regex                                  | X·ª≠ l√Ω t·ªët h∆°n `Tokenizer`                  |
| `StopWordsRemover` | Lo·∫°i b·ªè c√°c t·ª´ d·ª´ng (a, the, of, ...)                   | D√πng sau Tokenizer                         |
| `HashingTF`        | Chuy·ªÉn danh s√°ch t·ª´ th√†nh vector ƒë·∫∑c tr∆∞ng b·∫±ng hashing | Vector h√≥a d·ªØ li·ªáu vƒÉn b·∫£n                 |
| `CountVectorizer`  | Vector h√≥a t·ª´ theo t·∫ßn su·∫•t (bag of words)              | C√≥ th·ªÉ h·ªçc t·ª´ d·ªØ li·ªáu                      |
| `IDF`              | T√≠nh tr·ªçng s·ªë TF-IDF t·ª´ ƒë·∫ßu ra c·ªßa CountVectorizer      | Lo·∫°i b·ªè t·ª´ ph·ªï bi·∫øn                        |
| `StringIndexer`    | Bi·∫øn chu·ªói th√†nh s·ªë (label encoding)                    | `"Male"` ‚Üí `0`, `"Female"` ‚Üí `1`           |
| `OneHotEncoder`    | Bi·∫øn s·ªë th·ª© t·ª± th√†nh vector nh·ªã ph√¢n                    | `1` ‚Üí `[0, 1, 0]`                          |
| `VectorAssembler`  | G·ªôp nhi·ªÅu c·ªôt ƒë·∫∑c tr∆∞ng th√†nh 1 vector                  | ƒê·∫ßu v√†o cho m√¥ h√¨nh                        |
| `StandardScaler`   | Chu·∫©n h√≥a d·ªØ li·ªáu (mean=0, std=1)                       | ƒê·∫∑c bi·ªát quan tr·ªçng v·ªõi m√¥ h√¨nh tuy·∫øn t√≠nh |
| `MinMaxScaler`     | ƒê∆∞a d·ªØ li·ªáu v·ªÅ \[0, 1]                                  |                                            |
| `PCA`              | Gi·∫£m chi·ªÅu d·ªØ li·ªáu                                      |                                            |
| `ChiSqSelector`    | Ch·ªçn ƒë·∫∑c tr∆∞ng b·∫±ng th·ªëng k√™ Chi-Square                 | D√πng cho b√†i to√°n ph√¢n lo·∫°i                |
```bash
from pyspark.ml.feature import Tokenizer
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("TokenizerExample").getOrCreate()
df = spark.createDataFrame([(0, "hello world"), (1, "machine learning is fun")], ["id", "text"])
tokenizer = Tokenizer(inputCol="text", outputCol="words")
tokenized = tokenizer.transform(df)
tokenized.show(truncate=False)
```
```bash
from pyspark.ml.feature import RegexTokenizer

regex_tokenizer = RegexTokenizer(inputCol="text", outputCol="words", pattern="\\W")
regex_tokenized = regex_tokenizer.transform(df)
regex_tokenized.show(truncate=False)
```
```bash
from pyspark.ml.feature import StopWordsRemover

remover = StopWordsRemover(inputCol="words", outputCol="filtered")
filtered_df = remover.transform(tokenized)
filtered_df.show(truncate=False)
```
```bash
from pyspark.ml.feature import HashingTF

hashingTF = HashingTF(inputCol="filtered", outputCol="features", numFeatures=20)
featurized = hashingTF.transform(filtered_df)
featurized.show(truncate=False)
```
```bash
from pyspark.ml.feature import CountVectorizer, IDF

cv = CountVectorizer(inputCol="filtered", outputCol="rawFeatures")
cv_model = cv.fit(filtered_df)
cv_featurized = cv_model.transform(filtered_df)

idf = IDF(inputCol="rawFeatures", outputCol="features")
idf_model = idf.fit(cv_featurized)
rescaled = idf_model.transform(cv_featurized)
rescaled.select("filtered", "features").show(truncate=False)
```
```bash
from pyspark.ml.feature import StringIndexer

df_gender = spark.createDataFrame([("Male",), ("Female",), ("Female",)], ["gender"])
indexer = StringIndexer(inputCol="gender", outputCol="genderIndex")
df_indexed = indexer.fit(df_gender).transform(df_gender)
df_indexed.show()
```
```bash
from pyspark.ml.feature import OneHotEncoder

encoder = OneHotEncoder(inputCols=["genderIndex"], outputCols=["genderVec"])
encoded = encoder.fit(df_indexed).transform(df_indexed)
encoded.show()
```
```bash
from pyspark.ml.feature import VectorAssembler

df_features = spark.createDataFrame([(1.0, 2.0, 3.0)], ["f1", "f2", "f3"])
assembler = VectorAssembler(inputCols=["f1", "f2", "f3"], outputCol="features")
assembled = assembler.transform(df_features)
assembled.show()
```
```bash
from pyspark.ml.feature import StandardScaler

scaler = StandardScaler(inputCol="features", outputCol="scaled", withMean=True, withStd=True)
scaler_model = scaler.fit(assembled)
scaled_data = scaler_model.transform(assembled)
scaled_data.show()
```
```bash
from pyspark.ml.feature import MinMaxScaler

scaler = MinMaxScaler(inputCol="features", outputCol="scaled")
model = scaler.fit(assembled)
scaled = model.transform(assembled)
scaled.show()
```
```bash
from pyspark.ml.feature import PCA

pca = PCA(k=2, inputCol="features", outputCol="pcaFeatures")
pca_model = pca.fit(assembled)
pca_result = pca_model.transform(assembled)
pca_result.select("pcaFeatures").show(truncate=False)
```
```bash
from pyspark.ml.feature import ChiSqSelector
from pyspark.ml.linalg import Vectors

df = spark.createDataFrame([
    (1, Vectors.dense([0.0, 0.0, 18.0]), 1.0),
    (2, Vectors.dense([1.0, 1.0, 5.0]), 0.0),
    (3, Vectors.dense([2.0, 1.0, 8.0]), 1.0)
], ["id", "features", "label"])

selector = ChiSqSelector(numTopFeatures=2, featuresCol="features", outputCol="selectedFeatures", labelCol="label")
selected = selector.fit(df).transform(df)
selected.select("features", "selectedFeatures").show()
```
2. Estimators (m√¥ h√¨nh hu·∫•n luy·ªán)

| M√¥ h√¨nh                  | M√¥ t·∫£                                 |
| ------------------------ | ------------------------------------- |
| `LogisticRegression`     | Ph√¢n lo·∫°i nh·ªã ph√¢n                    |
| `LinearRegression`       | H·ªìi quy tuy·∫øn t√≠nh                    |
| `DecisionTreeClassifier` | C√¢y quy·∫øt ƒë·ªãnh ph√¢n lo·∫°i              |
| `RandomForestClassifier` | R·ª´ng ng·∫´u nhi√™n                       |
| `GBTClassifier`          | Gradient Boosted Tree                 |
| `NaiveBayes`             | Ph√¢n lo·∫°i Naive Bayes                 |
| `KMeans`                 | Ph√¢n c·ª•m K-Means                      |
| `FPGrowth`               | Khai ph√° lu·∫≠t k·∫øt h·ª£p                 |
| `ALS`                    | Collaborative filtering (recommender) |
```bash
from pyspark.ml.classification import LogisticRegression
from pyspark.sql import SparkSession
from pyspark.ml.linalg import Vectors
from pyspark.sql import Row

spark = SparkSession.builder.appName("LogisticRegression").getOrCreate()

data = [Row(label=0.0, features=Vectors.dense(1.0, 0.1)),
        Row(label=1.0, features=Vectors.dense(2.0, 1.1)),
        Row(label=0.0, features=Vectors.dense(1.3, 0.2))]

df = spark.createDataFrame(data)
lr = LogisticRegression(featuresCol="features", labelCol="label")
model = lr.fit(df)
model.transform(df).show()
```
```bash
from pyspark.ml.regression import LinearRegression

data = [(1.0, Vectors.dense(1.0)), (2.0, Vectors.dense(2.0)), (3.0, Vectors.dense(3.0))]
df = spark.createDataFrame(data, ["label", "features"])
lr = LinearRegression()
model = lr.fit(df)
model.transform(df).show()
```
```bash
from pyspark.ml.classification import DecisionTreeClassifier

data = [(0.0, Vectors.dense(0.0, 1.0)),
        (1.0, Vectors.dense(1.0, 0.0)),
        (0.0, Vectors.dense(0.0, 0.0))]
df = spark.createDataFrame(data, ["label", "features"])
dt = DecisionTreeClassifier()
model = dt.fit(df)
model.transform(df).show()
```
```bash
from pyspark.ml.classification import RandomForestClassifier

rf = RandomForestClassifier(numTrees=10)
model = rf.fit(df)
model.transform(df).show()
```
```bash
from pyspark.ml.classification import GBTClassifier

gbt = GBTClassifier()
model = gbt.fit(df)
model.transform(df).show()
```
```bash
from pyspark.ml.classification import NaiveBayes

df_nb = spark.createDataFrame([
    (0.0, Vectors.dense([1.0, 0.0])),
    (1.0, Vectors.dense([0.0, 1.0])),
    (0.0, Vectors.dense([1.0, 0.2]))
], ["label", "features"])

nb = NaiveBayes()
model = nb.fit(df_nb)
model.transform(df_nb).show()
```
```bash
from pyspark.ml.clustering import KMeans

df_kmeans = spark.createDataFrame([
    (Vectors.dense([1.0, 1.0]),),
    (Vectors.dense([9.0, 8.0]),),
    (Vectors.dense([1.2, 1.1]),),
], ["features"])

kmeans = KMeans(k=2)
model = kmeans.fit(df_kmeans)
model.transform(df_kmeans).show()
```
```bash
from pyspark.ml.fpm import FPGrowth

df_fpg = spark.createDataFrame([
    (0, ["milk", "bread"]),
    (1, ["milk", "diaper", "beer", "bread"]),
    (2, ["milk", "diaper", "bread", "cola"]),
], ["id", "items"])

fp = FPGrowth(itemsCol="items", minSupport=0.5, minConfidence=0.6)
model = fp.fit(df_fpg)
model.freqItemsets.show()
model.associationRules.show()
```
```bash
from pyspark.ml.recommendation import ALS

ratings = spark.createDataFrame([
    (0, 0, 4.0),
    (0, 1, 2.0),
    (1, 1, 3.0),
    (1, 2, 4.0),
], ["user", "item", "rating"])

als = ALS(userCol="user", itemCol="item", ratingCol="rating", coldStartStrategy="drop")
model = als.fit(ratings)
model.recommendForAllUsers(2).show(truncate=False)
```
3. Pipeline & Evaluation
   
| Th√†nh ph·∫ßn                               | M√¥ t·∫£                                                |
| ---------------------------------------- | ---------------------------------------------------- |
| `Pipeline`                               | Chu·ªói c√°c b∆∞·ªõc x·ª≠ l√Ω t·ª´ Transformer ƒë·∫øn Estimator    |
| `PipelineModel`                          | M√¥ h√¨nh pipeline sau khi fit                         |
| `TrainValidationSplit`, `CrossValidator` | T√°ch t·∫≠p hu·∫•n luy·ªán/ki·ªÉm tra ƒë·ªÉ t√¨m tham s·ªë t·ªët nh·∫•t |
| `BinaryClassificationEvaluator`          | ƒê√°nh gi√° m√¥ h√¨nh nh·ªã ph√¢n                            |
| `MulticlassClassificationEvaluator`      | ƒê√°nh gi√° ƒëa l·ªõp                                      |
| `RegressionEvaluator`                    | ƒê√°nh gi√° h·ªìi quy                                     |

**‚úÖ B√†i to√°n: Ph√¢n lo·∫°i nh·ªã ph√¢n vƒÉn b·∫£n (ham vs spam) v·ªõi LogisticRegression**
```bash
# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
from pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit
```
```bash
# T·∫°o SparkSession v√† d·ªØ li·ªáu m·∫´u
spark = SparkSession.builder.appName("TextClassificationPipeline").getOrCreate()

data = spark.createDataFrame([
    (0, "free money now", 1),
    (1, "hi how are you", 0),
    (2, "win big prize", 1),
    (3, "hello let's meet", 0),
], ["id", "text", "label"])
```
```bash
# X√¢y d·ª±ng Pipeline
tokenizer = Tokenizer(inputCol="text", outputCol="words")
stopwords = StopWordsRemover(inputCol="words", outputCol="filtered")
hashingTF = HashingTF(inputCol="filtered", outputCol="rawFeatures", numFeatures=20)
idf = IDF(inputCol="rawFeatures", outputCol="features")
lr = LogisticRegression(featuresCol="features", labelCol="label")

pipeline = Pipeline(stages=[tokenizer, stopwords, hashingTF, idf, lr])
```
```bash
# TrainValidationSplit ƒë·ªÉ t√¨m tham s·ªë t·ªët nh·∫•t
paramGrid = ParamGridBuilder() \
    .addGrid(hashingTF.numFeatures, [10, 20, 30]) \
    .addGrid(lr.regParam, [0.1, 0.01]) \
    .build()

evaluator = BinaryClassificationEvaluator(labelCol="label")

tvs = TrainValidationSplit(estimator=pipeline,
                           estimatorParamMaps=paramGrid,
                           evaluator=evaluator,
                           trainRatio=0.8)
```
```bash
# Hu·∫•n luy·ªán m√¥ h√¨nh & ƒë√°nh gi√°
model = tvs.fit(data)  # ƒê√¢y l√† PipelineModel

result = model.transform(data)
result.select("id", "text", "label", "probability", "prediction").show()

auc = evaluator.evaluate(result)
print("AUC =", auc)
```
üîÅ N·∫øu l√† ph√¢n lo·∫°i ƒëa l·ªõp: d√πng MulticlassClassificationEvaluator  
üîÅ N·∫øu l√† h·ªìi quy: d√πng RegressionEvaluator
4. H√†m ti·ªán √≠ch / ki·ªÉm tra

| H√†m            | M√¥ t·∫£                                 |
| -------------- | ------------------------------------- |
| `.fit()`       | Hu·∫•n luy·ªán m√¥ h√¨nh                    |
| `.transform()` | √Åp d·ª•ng m√¥ h√¨nh ho·∫∑c transformer      |
| `.evaluate()`  | ƒê√°nh gi√° hi·ªáu su·∫•t                    |
| `.getStages()` | L·∫•y danh s√°ch c√°c b∆∞·ªõc trong pipeline |
| `.randomSplit()| Chia d·ªØ li·ªáu th√†nh train/test         |
```bash
train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)
```
